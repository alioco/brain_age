{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BrainAge Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import LinearSVR\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_regression\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from nilearn.input_data import NiftiMasker\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# Config\n",
    "toy = False\n",
    "run_all = False\n",
    "\n",
    "path = '/home/ubuntu/fmriprep/'\n",
    "output_dir = '/home/ubuntu/output/' #AWS cloud\n",
    "# output_dir = 'data/CNP_T1/' #For Daniel's computer\n",
    "\n",
    "n_jobs = 20 #amount of cores\n",
    "cv=4\n",
    "\n",
    "description = 'CNP_T1_validation_age_'\n",
    "log_file = description+datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load subject ids\n",
    "if run_all:\n",
    "    files = os.listdir(path)\n",
    "    subj = [n[4:9] for n in files if n.startswith('sub')] #here we will filter the file names to leave only subj number as bellow\n",
    "\n",
    "    input_dir = '/home/ubuntu/'\n",
    "    file = 'subjlist_wfiles_CNP.txt'\n",
    "    with open(input_dir+file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    subj = [n.strip()[4:] for n in lines]\n",
    "\n",
    "#     This is only done once\n",
    "#     random_subj = subj[:]\n",
    "#     random.shuffle(random_subj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ages\n",
    "# df = pd.read_csv('participants.tsv', sep='\\t')\n",
    "\n",
    "# # take age from subj id\n",
    "# y_age = []\n",
    "# for i in random_subj:\n",
    "#     a = df.loc[df['participant_id'] == 'sub-'+i] #whole row\n",
    "#     y_age.append(int(a.age))\n",
    "\n",
    "\n",
    "# # Load IQ\n",
    "# df = pd.read_csv('add file here', sep='\\t')\n",
    "\n",
    "# # take age from subj id\n",
    "# y_age = []\n",
    "# for i in subj:\n",
    "#     a = df.loc[df['participant_id'] == 'sub-'+i] #whole row\n",
    "#     y_iq.append(int(a.id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load volumes for each subject into a dictionary\n",
    "# d = {}\n",
    "# for i in range(len(subj)):\n",
    "#     img = nib.load(path+'sub-'+subj[i]+'_T1w_space_MNI152NLin2009cAsym_preproc.nii.gz').get_fdata()\n",
    "#     d[subj[i]] = img\n",
    "\n",
    "if run_all:\n",
    "    filenames = []\n",
    "    # data = []\n",
    "    for i in range(len(random_subj)):\n",
    "    #     img = nib.load(path+'sub-'+subj[i]+/anat/'sub-'+subj[i]+'_T1w_space_MNI152NLin2009cAsym_preproc.nii.gz').get_fdata()\n",
    "    #     data.append(img)\n",
    "        filenames.append(path+'sub-'+random_subj[i]+'/anat/sub-'+random_subj[i]+'_T1w_space-MNI152NLin2009cAsym_preproc.nii.gz')\n",
    "\n",
    "# data  = np.array(data) \n",
    "\n",
    "## Let's see the size of a single volume\n",
    "# data.shape\n",
    "\n",
    "# # Flatten 3D volume and append to a list (or feed 3D volume to nilearn)\n",
    "# X = []\n",
    "\n",
    "# for i in d.values():\n",
    "#     flattened = i.flatten()\n",
    "#     X.append(flattened)\n",
    "    \n",
    "# # Turn list into array \n",
    "# X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affine resample\n",
    "\n",
    "# from nilearn.image import resample_to_img\n",
    "# # img = data.affine\n",
    "\n",
    "# resampled_imgs = []\n",
    "# resampled_imgs.append(nib.load(path+filenames[0]))\n",
    "                      \n",
    "# for i in range(1,len(filenames)):\n",
    "#     resampled_img = resample_to_img(path+filenames[i], path+filenames[0])\n",
    "#     resampled_imgs.append(resampled_img)\n",
    "\n",
    "# for i in resampled_imgs:\n",
    "#     print(i.affine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_all:\n",
    "    path_and_filenames = [path+n for n in filenames]\n",
    "\n",
    "    nifti_masker = NiftiMasker(\n",
    "        standardize=False,\n",
    "        smoothing_fwhm=2, mask_strategy='epi',\n",
    "        memory='nilearn_cache')  # cache options\n",
    "    gm_maps_masked = nifti_masker.fit_transform(filenames)\n",
    "    n_samples, n_features = gm_maps_masked.shape\n",
    "    print(\"%d samples, %d features\" % (n_samples, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_all:\n",
    "    print(\"%d samples, %d features\" % (n_samples, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_all:\n",
    "    %matplotlib inline\n",
    "    from nilearn import plotting\n",
    "    plotting.plot_roi(nifti_masker.mask_img_)\n",
    "    plotting.plot_roi(nifti_masker.mask_img_, bg_img=filenames[0])\n",
    "    plt.savefig(output_dir+'nilearn_mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_all:\n",
    "    X_train = gm_maps_masked[:84]\n",
    "    X_test = gm_maps_masked[84:]\n",
    "    y_train = y_age[:84]\n",
    "    y_test = y_age[84:]\n",
    "    random_subj_train = random_subj[:84]\n",
    "    random_subj_test = random_subj[84:]\n",
    "\n",
    "    np.savez_compressed(output_dir+log_file+'train_set',a=X_train, b=y_train, c=random_subj_train)\n",
    "    np.savez_compressed(output_dir+log_file+'test_set',a=X_test, b=y_test, c=random_subj_test)\n",
    "else:\n",
    "    loaded = np.load(output_dir+'train_set.npz')\n",
    "    X_train, y_train, random_subj_train = loaded['a'], loaded['b'], loaded['c']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84,)\n",
      "(84, 1899247)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch \n",
    "### Models\n",
    "* Linear Support vector regressor\n",
    "    * C = 0.0001, 0.001, 0.1, 1.0, 10.0\n",
    "\n",
    "### Feature selection\n",
    "* ANOVA\n",
    "    * 2000 features\n",
    "    * 50 k\n",
    "    * 100k features\n",
    "    * 500k\n",
    "    * Use all features (176*256*256=11534336)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#toy = False\n",
    "#if toy:\n",
    "#    X_train = X_train[:8]\n",
    " #   y_train = y_train[:8]\n",
    "\n",
    "\n",
    "# Remove features with too low between-subject variance\n",
    "# variance_threshold = \n",
    "\n",
    "# Here we use a classical univariate feature selection based on F-test,\n",
    "# namely Anova.\n",
    "# feature_selection = SelectKBest(f_regression)\n",
    "\n",
    "# We have our predictor (SVR), our feature selection (SelectKBest), and now,\n",
    "# we can plug them together in a *pipeline* that performs the two operations\n",
    "# successively:\n",
    "               \n",
    "               \n",
    "\n",
    "anova_svr = Pipeline([\n",
    "    ('anova', SelectKBest(f_regression)),\n",
    "    ('svr', LinearSVR())])\n",
    "\n",
    "parameters = [{'anova__k': [5000,100000, 'all'],\n",
    "               'svr__C': [0.00001, 0.0001, 0.001, 0.01, 1, 10]}]\n",
    "if toy:\n",
    "    parameters = [{'anova__k': [5000],'svr__C': [0.00001, 0.0001]}]\n",
    "\n",
    "\n",
    "grid = GridSearchCV(anova_svr, cv=cv, n_jobs=n_jobs, param_grid=parameters, scoring='neg_mean_absolute_error')\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# mean_scores = np.array(grid.cv_results_['neg_mean_absolute_error'])\n",
    "# mean_scores = mean_scores.reshape(len(C_OPTIONS), -1, len(N_FEATURES_OPTIONS))\n",
    "# # select score for best C\n",
    "# mean_scores = mean_scores.max(axis=0)\n",
    "# bar_offsets = (np.arange(len(N_FEATURES_OPTIONS)) *\n",
    "#                (len(reducer_labels) + 1) + .5)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('best score: ',grid.best_score_)\n",
    "print('best params: ',grid.best_params_)\n",
    "print('\\n\\nfullresults: \\n', grid.cv_results_)\n",
    "df1 = pd.DataFrame(grid.cv_results_)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_pred = grid.predict(X_train)\n",
    "mean_abs_error = np.mean(np.abs(age_pred-y_train))\n",
    "df1 = pd.DataFrame(grid.cv_results_)\n",
    "\n",
    "with open(output_dir+'log_'+log_file+'.txt', '+a') as f:\n",
    "    f.write('best score: '+str(grid.best_score_))\n",
    "    f.write('best params: '+str(grid.best_params_))\n",
    "    f.write('\\n\\nfullresults: \\n'+str(grid.cv_results_)+'\\n')\n",
    "    f.write(str(df1)+'\\n')\n",
    "    f.write('predictions: '+str(age_pred)+'\\n')\n",
    "    f.write('true ages: '+str(y_train)+'\\n')\n",
    "    f.write(str(mean_abs_error)+'\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir+'log_'+log_file+'.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
